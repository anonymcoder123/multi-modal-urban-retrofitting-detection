{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Retrofitting Detection using Multi-Modal Temporal Vision Transformer **(Ablation - No Demographic Component)**\n",
    "\n",
    "This notebook implements a deep learning pipeline for detecting urban retrofitting changes using temporal street view images and demographic data. The model combines Vision Transformer (ViT) features from before/after images **without demographic data** to classify urban changes into five categories.\n",
    "\n",
    "## Table of Contents\n",
    "1. Installation and Setup\n",
    "2. Package Imports\n",
    "3. System Configuration\n",
    "4. Dataset and Model Architecture\n",
    "5. Training and Validation Functions\n",
    "6. Configuration and Hyperparameters\n",
    "7. Data Loading and Preprocessing\n",
    "8. Data Visualization\n",
    "9. Model Training\n",
    "10. Evaluation and Results\n",
    "11. Prediction Export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "Install required Python packages for deep learning, computer vision, and geospatial analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install python dependencies\n",
    "\n",
    "# %pip install torch torchvision timm pandas geopandas numpy opencv-python scikit-learn matplotlib folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Package Imports\n",
    "\n",
    "Import all necessary libraries for data processing, model training, visualization, and geospatial operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python packages\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import folium\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Configuration\n",
    "\n",
    "### 3.1 GPU Availability Check\n",
    "\n",
    "Check NVIDIA GPU availability and specifications for model training acceleration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Memory Management\n",
    "\n",
    "Clear GPU memory and cache to free up resources before starting training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory and cache\n",
    "\n",
    "# del model, optimizer, images, labels, outputs\n",
    "# del model, optimizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Device Configuration\n",
    "\n",
    "Set the computation device (CUDA GPU or CPU) for PyTorch operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device for torch backend\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and Model Architecture\n",
    "\n",
    "### 4.1 Custom Dataset Class\n",
    "\n",
    "Define the `UrbanRetrofittingDataset` class that loads temporal image pairs (before/after) and labels. The dataset handles coordinate precision, image loading, and label encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure images, deomgraphic data and labels.\n",
    "\n",
    "class UrbanRetrofittingDataset(Dataset):\n",
    "    def __init__(self, excel_file, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            excel_file (str): Path to the Excel file with annotations.\n",
    "            image_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image pair.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_excel(excel_file, dtype={\"Latitude\": str, \"Longitude\": str})\n",
    "        # maintains precision. Do not change this two step converesion.\n",
    "        self.data = self.data.astype({\"Latitude\": np.longdouble, \"Longitude\": np.longdouble})\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['Change'] = self.label_encoder.fit_transform(self.data['Change'])  # Encode labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        census_block = str(row['ID'])\n",
    "        lat, lon, direction = row['Latitude'], row['Longitude'], row['Heading']\n",
    "        start_date, end_date = row['Start'], row['End']\n",
    "        label = row['Change']\n",
    "\n",
    "        # Load image pair Do not use f{} formatting to add longdouble number\n",
    "        start_image_path = os.path.join(self.image_dir, census_block,  str(lat)+\"_\"+str(lon)+\"_\"+str(direction), f\"{pd.to_datetime(start_date).strftime('%b %Y')}.png\")\n",
    "        end_image_path = os.path.join(self.image_dir, census_block, str(lat)+\"_\"+str(lon)+\"_\"+str(direction), f\"{pd.to_datetime(end_date).strftime('%b %Y')}.png\")\n",
    "        start_image = Image.open(start_image_path).convert('RGB')\n",
    "        end_image = Image.open(end_image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            start_image = self.transform(start_image)\n",
    "            end_image = self.transform(end_image)\n",
    "\n",
    "        return start_image, end_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Vision Transformer Model Architecture\n",
    "\n",
    "Implement the `ViTWithSocioEconomic` model that combines:\n",
    "- Pretrained ViT for extracting image features from start and end images\n",
    "- Difference features (end - start) to capture temporal changes\n",
    "- Classification head for 5-class urban retrofitting detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "\n",
    "class ViTWithSocioEconomic(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ViTWithSocioEconomic, self).__init__()\n",
    "        # Pretrained ViT model from timm library without classification head to get image embeddings.\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "        img_embedding_dim = self.vit.num_features\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3 * img_embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, start_image, end_image):\n",
    "        start_feat = self.vit(start_image)\n",
    "        end_feat = self.vit(end_image)\n",
    "        diff_feat = end_feat - start_feat\n",
    "        combined = torch.cat([start_feat, end_feat, diff_feat], dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and Validation Functions\n",
    "\n",
    "### 5.1 Training Function\n",
    "\n",
    "Implement the training loop with per-class accuracy and loss tracking. This function processes batches, computes gradients, and updates model parameters while monitoring performance metrics for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with per-class accuracy and loss tracking\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, num_classes):\n",
    "    model.train()\n",
    "    total_samples = 0\n",
    "    running_loss = 0.0\n",
    "    correct_total = 0\n",
    "    \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    class_losses = [0.0] * num_classes\n",
    "    \n",
    "    for start_imgs, end_imgs, labels in loader:\n",
    "        start_imgs, end_imgs = start_imgs.to(device), end_imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(start_imgs, end_imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct_total += (preds == labels).sum().item()\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            cls_mask = labels == cls\n",
    "            if cls_mask.sum() > 0:\n",
    "                cls_outputs = outputs[cls_mask]\n",
    "                cls_labels = labels[cls_mask]\n",
    "                class_losses[cls] += criterion(cls_outputs, cls_labels).item() * cls_mask.sum().item()\n",
    "                class_correct[cls] += (cls_outputs.argmax(dim=1) == cls_labels).sum().item()\n",
    "                class_total[cls] += cls_mask.sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    overall_acc = 100 * correct_total / total_samples\n",
    "\n",
    "    class_acc = [100 * class_correct[i] / class_total[i] if class_total[i] else 0 for i in range(num_classes)]\n",
    "    class_avg_loss = [class_losses[i] / class_total[i] if class_total[i] else 0 for i in range(num_classes)]\n",
    "\n",
    "    return epoch_loss, overall_acc, class_acc, class_avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Validation Function\n",
    "\n",
    "Implement validation with top-2 accuracy metric. This function evaluates the model on validation data, computes per-class metrics, and stores predictions for later analysis. Top-2 accuracy considers a prediction correct if the true label is among the top 2 predicted classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function with top-2 accuracy and per-class metrics\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    running_loss = 0.0\n",
    "    correct_total = 0\n",
    "\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    class_losses = [0.0] * num_classes\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start_imgs, end_imgs, labels in loader:\n",
    "            start_imgs, end_imgs = start_imgs.to(device), end_imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(start_imgs, end_imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            _, preds_top2 = outputs.topk(2, dim=1)\n",
    "            correct_top2 = preds_top2.eq(labels.unsqueeze(1)).any(dim=1)\n",
    "            stored_pred_batch = torch.where(correct_top2, labels, preds_top2[:, 0])\n",
    "\n",
    "            all_preds.extend(stored_pred_batch.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct_total += correct_top2.sum().item()\n",
    "\n",
    "            for cls in range(num_classes):\n",
    "                cls_mask = labels == cls\n",
    "                if cls_mask.sum() > 0:\n",
    "                    cls_outputs = outputs[cls_mask]\n",
    "                    cls_labels = labels[cls_mask]\n",
    "                    class_losses[cls] += criterion(cls_outputs, cls_labels).item() * cls_mask.sum().item()\n",
    "                    class_correct[cls] += correct_top2[cls_mask].sum().item()\n",
    "                    class_total[cls] += cls_mask.sum().item()\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    overall_acc = 100 * correct_total / total_samples\n",
    "\n",
    "    class_acc = [100 * class_correct[i] / class_total[i] if class_total[i] else 0 for i in range(num_classes)]\n",
    "    class_avg_loss = [class_losses[i] / class_total[i] if class_total[i] else 0 for i in range(num_classes)]\n",
    "\n",
    "    return epoch_loss, overall_acc, class_acc, class_avg_loss, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration and Hyperparameters\n",
    "\n",
    "Set up directories for checkpoints and logs, define model name, and configure hyperparameters including batch size, learning rate, number of epochs, and dataset paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for saving and loading checkpoints.\n",
    "checkpoint_dir = \"ViT_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "MODEL_NAME = \"ablated_demographic_component_model\"\n",
    "CHECKPOINT_PATH = f\"{checkpoint_dir}/{MODEL_NAME}_epoch_30.pth\" \n",
    "# directory for storing logs\n",
    "logs_dir = \"logs\"\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "LOG_FILE = f\"{logs_dir}/{MODEL_NAME}.txt\"\n",
    "GRAPH_PATH = f\"Model Graphs/{MODEL_NAME}.png\"\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY_RATE = 0.00001\n",
    "NUM_CLASSES = 5  # Five aspects of urban retrofitting\n",
    "\n",
    "# Dataset and DataLoader\n",
    "excel_File = \"labels.xlsx\"\n",
    "image_dir = \"Streetview_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Logging Configuration\n",
    "\n",
    "Configure logging to write training progress to both a file and console for monitoring and debugging purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to file and console\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        # Append mode\n",
    "        logging.FileHandler(LOG_FILE, mode='a'),\n",
    "        # Also print to console\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Loading and Preprocessing\n",
    "\n",
    "### 7.1 Data Transformations and Spatial Stratified Split\n",
    "\n",
    "Load the dataset with appropriate transformations:\n",
    "- **Training transforms**: Include data augmentation (random flips, rotations, color jitter) to improve generalization\n",
    "- **Validation transforms**: Only resize and normalize (no augmentation)\n",
    "\n",
    "Implement spatial stratified sampling to ensure:\n",
    "- Geographic diversity: Samples are split across spatial clusters\n",
    "- Class balance: Each cluster maintains proportional class distribution\n",
    "- Reproducibility: Fixed random seed for consistent splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Data transformations\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Spatial stratified sampling split function \n",
    "\n",
    "def spatial_stratified_split(dataset, val_ratio=0.2, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Perform spatial stratified sampling split and return PyTorch-compatible UrbanRetrofittingDataset subsets.\n",
    "    NOTE: n_clusters = 4 included at least 1 sample per class in each cluster. \n",
    "    Args:\n",
    "        df (pd.DataFrame): Raw dataframe loaded from Excel.\n",
    "        image_dir (str): Path to the image directory.\n",
    "        transform (callable): Torchvision transform for images.\n",
    "        val_ratio (float): Ratio for validation split.\n",
    "        n_clusters (int): Number of spatial clusters.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (Subset), val_dataset (Subset), label_encoder\n",
    "    \"\"\"\n",
    "    df = dataset.data.copy()\n",
    "    \n",
    "    # Ensure correct types\n",
    "    df = df.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "    # Spatial clustering\n",
    "    coords = df[['Latitude', 'Longitude']].values\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df['Cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "    # Stratified sampling within each spatial cluster\n",
    "    train_indices, val_indices = [], []\n",
    "\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        cluster_df = df[df['Cluster'] == cluster]\n",
    "        for label in cluster_df['Change'].unique():\n",
    "            label_df = cluster_df[cluster_df['Change'] == label]\n",
    "            n_val = max(1, int(len(label_df) * val_ratio))\n",
    "            val_sample = label_df.sample(n=n_val, random_state=42)\n",
    "            train_sample = label_df.drop(val_sample.index)\n",
    "            train_indices.extend(train_sample.index)\n",
    "            val_indices.extend(val_sample.index)\n",
    "\n",
    "    # Reset index to use Subset\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    dataset.data = df\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Longitude'], df['Latitude']), crs='EPSG:4326')\n",
    "\n",
    "    return train_indices, val_indices, gdf\n",
    "\n",
    "logger.info(\"Data Loading Initiated!\")\n",
    "# initializing dataset twice to use multiple transforms for training and validation.\n",
    "train_dataset = UrbanRetrofittingDataset(excel_File, image_dir, train_transform)\n",
    "val_dataset = UrbanRetrofittingDataset(excel_File, image_dir, val_transform)\n",
    "\n",
    "# use any one to get train and val indices.\n",
    "train_indices, val_indices, clusters_gdf = spatial_stratified_split(dataset=train_dataset, val_ratio=0.2, n_clusters=20)\n",
    "_,_,_ = spatial_stratified_split(dataset=val_dataset, val_ratio=0.2, n_clusters=20)\n",
    "\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "val_dataset = Subset(val_dataset, val_indices)\n",
    "logger.info(\"Split data into training and validation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Dataset Distribution Logging\n",
    "\n",
    "Log the distribution of classes in training and validation sets to verify balanced splits and understand data composition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log cluster distribution\n",
    "\n",
    "all_labels = train_dataset.dataset.data['Change']\n",
    "\n",
    "def log_distribution(all_labels, dataset, name):\n",
    "    label_counts = Counter([all_labels[i] for i in dataset.indices])\n",
    "    label_encoder = dataset.dataset.label_encoder\n",
    "    logger.info(f\"{name} distribution:\")\n",
    "    for label_idx in sorted(label_counts):\n",
    "        logger.info(f\"\\t{label_encoder.classes_[label_idx]}: {label_counts[label_idx]}\")\n",
    "\n",
    "log_distribution(all_labels, train_dataset, \"Train\")\n",
    "log_distribution(all_labels, val_dataset, \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Training Set Cluster Analysis\n",
    "\n",
    "Display the distribution of samples across spatial clusters and classes in the training set. This helps verify that the spatial stratified split maintains class representation in each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of clusters and class distribution for training set\n",
    "\n",
    "train_df = train_dataset.dataset.data.loc[train_indices]\n",
    "counts = train_df.groupby(['Cluster', 'Change']).size().unstack(fill_value=0)\n",
    "logger.info(\"Training Sample counts per class in each cluster:\")\n",
    "logger.info(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Validation Set Cluster Analysis\n",
    "\n",
    "Display the distribution of samples across spatial clusters and classes in the validation set to ensure proper validation coverage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of clusters and class distribution for validation set\n",
    "\n",
    "val_df = val_dataset.dataset.data.loc[val_indices]\n",
    "counts = val_df.groupby(['Cluster', 'Change']).size().unstack(fill_value=0)\n",
    "logger.info(\"Validation Sample counts per class in each cluster:\")\n",
    "logger.info(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Visualization\n",
    "\n",
    "### 8.1 Color Mapping Setup\n",
    "\n",
    "Prepare color schemes for visualizing different urban retrofitting classes on maps. Each class is assigned a distinct color for easy identification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "# Prepare colors for classes\n",
    "classes = train_dataset.dataset.label_encoder.classes_\n",
    "num_classes = len(classes)\n",
    "# colormap = cm.get_cmap('tab10', num_classes)\n",
    "# class_colors = {classes[i]: mcolors.rgb2hex(colormap(i)) for i in range(num_classes)}\n",
    "class_colors = {\n",
    "    'No Change': \"#9caab6\",\n",
    "    're-building': \"#8243ab\",\n",
    "    're-capital': '#2e8453',\n",
    "    're-inhabitation': '#08478c',\n",
    "    're-transportation': \"#b82200\"\n",
    "}\n",
    "\n",
    "# Get cluster polygons (convex hull)\n",
    "cluster_polys = clusters_gdf[['Cluster', 'geometry']].dissolve(by='Cluster', as_index=False, aggfunc='first')\n",
    "cluster_polys['geometry'] = cluster_polys.geometry.convex_hull\n",
    "cluster_polys = gpd.GeoDataFrame(cluster_polys, geometry=\"geometry\", crs='EPSG:4326')\n",
    "cluster_polys.rename(columns={0: 'geometry'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Interactive Map Creation\n",
    "\n",
    "Create interactive Folium maps showing:\n",
    "- Spatial clusters as polygons (convex hulls)\n",
    "- Sample locations colored by class\n",
    "- Legend for class identification\n",
    "\n",
    "This visualization helps understand the geographic distribution of training and validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build map\n",
    "def create_folium_cluster_map(data_gdf, class_colors, title):\n",
    "    m = folium.Map(location=[data_gdf['Latitude'].mean(), data_gdf['Longitude'].mean()], tiles=\"Cartodb Positron\", zoom_start=11, zoom_control=False)\n",
    "    \n",
    "    # Plot clusters (polygons)\n",
    "    for _, row in cluster_polys.iterrows():\n",
    "        folium.GeoJson(\n",
    "            row['geometry'],\n",
    "            style_function=lambda feature, clr=row['Cluster']: {\n",
    "                'fillColor': \"#e7e7e7\",\n",
    "                'color': 'black',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': 0.8\n",
    "            },\n",
    "            tooltip=f\"Cluster {row['Cluster']}\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add class samples as points\n",
    "    for _, row in data_gdf.iterrows():\n",
    "        cls = row['Change']\n",
    "        color = class_colors[classes[cls]]\n",
    "        popup = f\"Class: {cls}<br>Cluster: {row['Cluster']}\"\n",
    "        folium.CircleMarker(\n",
    "            location=[row['Latitude'], row['Longitude']],\n",
    "            radius=2.5,\n",
    "            color=\"white\",      # Outline color\n",
    "            weight=0.5,           # Outline thickness\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=1,\n",
    "            popup=popup\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add legend\n",
    "    legend_html = \"<div style='position: fixed; top: 20px; right: 20px; z-index: 9999; background-color: white; padding: 10px; border: 1px solid #ccc;'>\"\n",
    "    legend_html += \"<b>Retrofit Category</b><br>\"\n",
    "    for cls, color in class_colors.items():\n",
    "        legend_html += f\"<i style='background:{color}; width:10px;height:10px;display:inline-block;border-radius:50%;'></i> {cls}<br>\"\n",
    "    legend_html += \"</div>\"\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    # # Add the Fullscreen plugin to the map\n",
    "    # Fullscreen().add_to(m)\n",
    "    return m\n",
    "\n",
    "# Generate and save maps\n",
    "train_map = create_folium_cluster_map(train_df, class_colors, \"Training Data Map\")\n",
    "val_map = create_folium_cluster_map(val_df, class_colors, \"Validation Data Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Training Set Map Visualization\n",
    "\n",
    "Display the interactive map of training samples with spatial clusters and class-colored points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training set map.\n",
    "\n",
    "train_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Validation Set Map Visualization\n",
    "\n",
    "Display the interactive map of validation samples with spatial clusters and class-colored points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize validation set map. \n",
    "\n",
    "val_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Training Set DataFrame Display\n",
    "\n",
    "Display the training dataset DataFrame for inspection of data structure and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display traing set dataframe.\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Validation Set DataFrame Display\n",
    "\n",
    "Display the validation dataset DataFrame for inspection of data structure and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display validation set dataframe.\n",
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Class Weights for Sampling\n",
    "\n",
    "Compute class weights for the weighted random sampler to address class imbalance during training. These weights ensure that underrepresented classes are sampled more frequently during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract labels from training dataset\n",
    "# train_labels = [sample[2] for sample in train_dataset]  # sample[3] is the label in __getitem__\n",
    "all_labels = train_dataset.dataset.data['Change'] #defined above already\n",
    "train_labels = [all_labels[i] for i in train_dataset.indices]\n",
    "num_classes = len(set(train_labels))\n",
    "\n",
    "# Compute class sample weights\n",
    "# class_counts = np.array([train_labels.count(i) for i in range(num_classes)])\n",
    "# class_weights_sampler = 1. / class_counts\n",
    "\n",
    "# Class sample weights calculated manually to avoid extreme weights for very rare classes.\n",
    "class_weights_sampler = np.array([0.0036, 0.0034, 0.0150, 0.0050, 0.0060]) # v21\n",
    "sample_weights = np.array([class_weights_sampler[label] for label in train_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 Weighted Random Sampler\n",
    "\n",
    "Create a weighted random sampler that uses the computed class weights to balance the training data during batch creation. This helps the model learn from all classes more evenly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted sampler for drawing training samples.\n",
    "sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "                                 num_samples=len(sample_weights),\n",
    "                                 replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9 Class Weights for Loss Function\n",
    "\n",
    "Compute class weights for the loss function to penalize misclassifications of rare classes more heavily. This helps the model focus on learning underrepresented classes during optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute class weights for loss function.\n",
    "# class_weights = compute_class_weight(\n",
    "#     class_weight='balanced',\n",
    "#     classes=np.unique(labels),\n",
    "#     y=labels\n",
    "# )\n",
    "\n",
    "# Manually calculated class weights to avoid extreme weights for very rare classes.\n",
    "class_weights = np.array([1.66, 7.22, 5.56, 9.22, 9.34]) #v21\n",
    "\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "logger.info(\"Class weights used for loss function:\")\n",
    "for label, weight in zip(train_dataset.dataset.label_encoder.classes_, class_weights):\n",
    "    logger.info(f\"\\tClass {label}: Weight={weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.10 Data Loading Workers Configuration\n",
    "\n",
    "Determine the optimal number of worker processes for parallel data loading. Using multiple workers speeds up data preprocessing and loading during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to enable faster data loading using multiple workers for data loading.\n",
    "\n",
    "import multiprocessing\n",
    "num_workers = multiprocessing.cpu_count() // 2\n",
    "num_workers\n",
    "# Above result was 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.11 DataLoader Initialization\n",
    "\n",
    "Create PyTorch DataLoaders for training and validation sets with:\n",
    "- Batch size configuration\n",
    "- Weighted sampling for training (to handle class imbalance)\n",
    "- Multiple workers for parallel data loading\n",
    "- Pinned memory for faster GPU transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders.\n",
    "\n",
    "logger.info(f\"Batch Size: {BATCH_SIZE}, Workers: {num_workers}, pin_memory: True, LR: {LEARNING_RATE}, Wt. Decay: {WEIGHT_DECAY_RATE}\")# this will be a lazy loader. Just initialization happens here. Real work happens inside for loop of data loader.\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=num_workers, pin_memory=True)\n",
    "logger.info(\"Training Loader is now ready!\")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "logger.info(\"Validation Loader is now ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training\n",
    "\n",
    "### 9.1 Training Loop with Checkpointing\n",
    "\n",
    "Initialize the model, loss function, and optimizer. The training loop:\n",
    "- Trains the model for specified number of epochs\n",
    "- Validates after each epoch with top-2 accuracy\n",
    "- Logs per-class and overall metrics\n",
    "- Saves checkpoints periodically (after epoch 18) for model recovery\n",
    "- Supports resuming from checkpoints if training is interrupted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ViTWithSocioEconomic(NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY_RATE)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    train_acc_per_class = checkpoint['train_acc_per_class']\n",
    "    val_acc_per_class = checkpoint['val_acc_per_class']\n",
    "    train_loss_per_class = checkpoint['train_loss_per_class']\n",
    "    val_loss_per_class = checkpoint['val_loss_per_class']\n",
    "    overall_train_acc = checkpoint['overall_train_acc']\n",
    "    overall_val_acc = checkpoint['overall_val_acc']\n",
    "    overall_train_loss = checkpoint['overall_train_loss']\n",
    "    overall_val_loss = checkpoint['overall_val_loss']\n",
    "    \n",
    "    val_preds = checkpoint['val_preds']\n",
    "    val_labels = checkpoint['val_labels']\n",
    "    \n",
    "    logger.info(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "else:\n",
    "    # Initialize lists to store losses and accuracies\n",
    "    start_epoch = 0\n",
    "    train_acc_per_class = []\n",
    "    val_acc_per_class = []\n",
    "    train_loss_per_class = []\n",
    "    val_loss_per_class = []\n",
    "    overall_train_acc = []\n",
    "    overall_val_acc = []\n",
    "    overall_train_loss = []\n",
    "    overall_val_loss = []\n",
    "    val_preds = None\n",
    "    val_labels = None\n",
    "\n",
    "    logger.info(\"No checkpoint found - starting new training\")\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    train_loss, train_overall_acc,  train_class_acc, train_class_loss = train_epoch(model, train_loader, criterion, optimizer, device, NUM_CLASSES)\n",
    "    val_loss, val_overall_acc, val_class_acc, val_class_loss, val_preds, val_labels  = validate_epoch(model, val_loader, criterion, device, NUM_CLASSES)\n",
    "    \n",
    "    train_acc_per_class.append(train_class_acc)\n",
    "    val_acc_per_class.append(val_class_acc)\n",
    "    train_loss_per_class.append(train_class_loss)\n",
    "    val_loss_per_class.append(val_class_loss)\n",
    "    overall_train_acc.append(train_overall_acc)\n",
    "    overall_val_acc.append(val_overall_acc)\n",
    "    overall_train_loss.append(train_loss)\n",
    "    overall_val_loss.append(val_loss)\n",
    "\n",
    "    logger.info(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    logger.info(f\"Train Accuracy: {train_overall_acc:.2f}%\")\n",
    "    logger.info(f\"Train Loss: {train_loss:.4f}\")\n",
    "    for idx, cls in enumerate(train_dataset.dataset.label_encoder.classes_):\n",
    "        logger.info(f\"\\tTrain Class '{cls}': Acc={train_class_acc[idx]:.2f}%, Loss={train_class_loss[idx]:.4f}\")\n",
    "    \n",
    "    logger.info(f\"Validation Accuracy: {val_overall_acc:.2f}%\")\n",
    "    logger.info(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    for idx, cls in enumerate(train_dataset.dataset.label_encoder.classes_):\n",
    "        logger.info(f\"\\tVal Class '{cls}': Acc={val_class_acc[idx]:.2f}%, Loss={val_class_loss[idx]:.4f}\")\n",
    "\n",
    "    # Save checkpoints every 2 epochs\n",
    "    # if (epoch + 1) % 2 == 0:\n",
    "    if (epoch + 1) >= 18:\n",
    "        CHECKPOINT_PATH = os.path.join(checkpoint_dir, f\"ablated_demographic_component_model_epoch_{epoch+1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_acc_per_class': train_acc_per_class,\n",
    "            'val_acc_per_class': val_acc_per_class,\n",
    "            'train_loss_per_class': train_loss_per_class,\n",
    "            'val_loss_per_class': val_loss_per_class,\n",
    "            'overall_train_acc':overall_train_acc,\n",
    "            'overall_val_acc':overall_val_acc,\n",
    "            'overall_train_loss':overall_train_loss,\n",
    "            'overall_val_loss':overall_val_loss,\n",
    "            'val_preds': val_preds,\n",
    "            'val_labels': val_labels\n",
    "    \n",
    "        }, CHECKPOINT_PATH)\n",
    "        logger.info(f\"Saved checkpoint to {CHECKPOINT_PATH}\")\n",
    "\n",
    "    logger.info(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation and Results\n",
    "\n",
    "### 10.1 Classification Report and Confusion Matrix\n",
    "\n",
    "Generate detailed classification metrics including:\n",
    "- Precision, recall, and F1-score for each class\n",
    "- Confusion matrix visualization showing prediction vs. actual labels\n",
    "- Overall model performance summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "classes = val_dataset.dataset.label_encoder.classes_\n",
    "print(classification_report(val_labels, val_preds, target_names=classes, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
