{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List corrupted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = \"Streetview_data\"\n",
    "\n",
    "# List to store corrupted files\n",
    "corrupted_files = []\n",
    "\n",
    "# Traverse through census block folders\n",
    "for census_block in os.listdir(root_dir):\n",
    "    census_block_path = os.path.join(root_dir, census_block)\n",
    "    \n",
    "    if os.path.isdir(census_block_path):  # Ensure it's a directory\n",
    "        # Traverse through location subfolders\n",
    "        for location in os.listdir(census_block_path):\n",
    "            location_path = os.path.join(census_block_path, location)\n",
    "\n",
    "            if os.path.isdir(location_path):  # Ensure it's a directory\n",
    "                # Check PNG files inside\n",
    "                for file in os.listdir(location_path):\n",
    "                    if file.endswith(\".png\"):\n",
    "                        file_path = os.path.join(location_path, file)\n",
    "                        if os.path.getsize(file_path) <= 1024:  # File size ≤ 1KB\n",
    "                            # Store path relative to Census block folder\n",
    "                            relative_path = os.path.relpath(file_path, root_dir)\n",
    "                            corrupted_files.append(relative_path)\n",
    "\n",
    "# Print the corrupted files\n",
    "if corrupted_files:\n",
    "    print(\"Corrupted PNG files (≤ 1KB):\")\n",
    "    for file in corrupted_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No corrupted PNG files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve pair of images which are according to the folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths\n",
    "base_dir = 'Streetview_data'\n",
    "input_excel = 'Streetview Image Label.xlsx'\n",
    "output_excel = 'revised_image_labels.xlsx'\n",
    "\n",
    "def find_matching_folder(census_block_id, lat, lon, heading):\n",
    "    \"\"\"\n",
    "    Find the best-matching folder inside the census block directory by comparing lat/lon up to the second-last digit.\n",
    "    \"\"\"\n",
    "    base_folder = os.path.join(base_dir, str(census_block_id))\n",
    "    if not os.path.exists(base_folder):\n",
    "        return None  # If the census block folder doesn't exist, return None\n",
    "\n",
    "    # Convert lat/lon to strings once\n",
    "    lat_str, lon_str = str(lat), str(lon)\n",
    "\n",
    "    # Precompute length for slicing\n",
    "    lat_len, lon_len = len(lat_str) - 1, len(lon_str) - 1\n",
    "\n",
    "    matching_folders = []\n",
    "\n",
    "    with os.scandir(base_folder) as entries:\n",
    "        directories = [entry for entry in entries if entry.is_dir()] # Skip files, only check directories\n",
    "        for entry in directories:\n",
    "            parts = entry.name.split(\"_\")\n",
    "            if len(parts) != 3: # escape invalid folders\n",
    "                continue \n",
    "            try:\n",
    "                folder_lat_str, folder_lon_str, folder_heading = str(parts[0]), str(parts[1]), int(parts[2])\n",
    "                \n",
    "                # Check up to second-last digit     \n",
    "                if (folder_lat_str[:lat_len] == lat_str[:lat_len] and \n",
    "                    folder_lon_str[:lon_len] == lon_str[:lon_len] and \n",
    "                    folder_heading == heading):\n",
    "                    matching_folders.append(entry.name)\n",
    "\n",
    "                    if len(matching_folders) > 1:\n",
    "                        print(entry.name)\n",
    "                        raise Exception(\"More than two matching folders found.\")\n",
    "\n",
    "            except ValueError:\n",
    "                continue  # Skip if conversion fails\n",
    "\n",
    "    return matching_folders[0]  # return first folder\n",
    "\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_excel, dtype={\"Latitude\": str, \"Longitude\": str})\n",
    "\n",
    "# Convert Start and End columns to datetime with day first\n",
    "df['Start'] = pd.to_datetime(df['Start'], dayfirst=True, errors='coerce').dt.date\n",
    "df['End'] = pd.to_datetime(df['End'], dayfirst=True, errors='coerce').dt.date\n",
    "\n",
    "# Prepare a list to collect valid records\n",
    "valid_records = []\n",
    "\n",
    "# Iterate through each record\n",
    "for index, row in df.iterrows():\n",
    "    census_block_id = row['ID']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "    heading = row['Heading']\n",
    "    start_date = datetime.strptime(str(row['Start']), '%Y-%m-%d').date()\n",
    "    end_date = datetime.strptime(str(row['End']), '%Y-%m-%d').date()\n",
    "    change = row['Change']\n",
    "\n",
    "    # Construct folder path\n",
    "    # folder_path = os.path.join(base_dir, str(census_block_id), f\"{latitude}_{longitude}_{heading}\")\n",
    "    # Find the correct folder even if lat/lon precision is different\n",
    "    folder = find_matching_folder(census_block_id, latitude, longitude, heading)\n",
    "    if folder is None:\n",
    "        raise FileNotFoundError(f\"No matching folder found for {latitude}, {longitude}, {heading} in Census Block {census_block_id}\")\n",
    "    folder_path = os.path.join(base_dir, str(census_block_id), folder)\n",
    "    # print(f\"Folder: {census_block_id}/{folder} Start: {start_date} End: {end_date} Change: {change}\")\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        # List all images in the folder\n",
    "        images = [img for img in os.listdir(folder_path) if img.endswith('.png')]\n",
    "\n",
    "        # Extract dates from image filenames\n",
    "        image_dates = {datetime.strptime(img.split('.')[0], '%b %Y').date(): img for img in images}\n",
    "\n",
    "        # Check if both start and end images are present\n",
    "        if start_date in image_dates and end_date in image_dates:\n",
    "            lat, lng, heading = folder.split(\"_\")\n",
    "            row['Latitude'] = lat\n",
    "            row['Longitude'] = lng\n",
    "            valid_records.append(row)\n",
    "        else:\n",
    "            print(f\"Folder: {census_block_id}/{folder} Start: {start_date} End: {end_date} Change: {change}\")\n",
    "\n",
    "# Save the valid records to a new Excel file\n",
    "valid_df = pd.DataFrame(valid_records)\n",
    "\n",
    "# Ensure Start and End columns are datetime objects without time\n",
    "valid_df['Start'] = pd.to_datetime(valid_df['Start'], dayfirst=True).dt.date\n",
    "valid_df['End'] = pd.to_datetime(valid_df['End'], dayfirst=True).dt.date\n",
    "\n",
    "# Write to Excel with date format\n",
    "with pd.ExcelWriter(output_excel, engine='xlsxwriter') as writer:\n",
    "    valid_df.to_excel(writer, index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "    # Apply date format without time\n",
    "    date_format = workbook.add_format({'num_format': 'mm/dd/yyyy'})\n",
    "    start_col_idx = valid_df.columns.get_loc('Start')\n",
    "    end_col_idx = valid_df.columns.get_loc('End')\n",
    "\n",
    "    # Apply format to columns\n",
    "    worksheet.set_column(start_col_idx, start_col_idx, 15, date_format)\n",
    "    worksheet.set_column(end_col_idx, end_col_idx, 15, date_format)\n",
    "\n",
    "print(f\"Validation complete. Saved {len(valid_records)} valid records to {output_excel}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make single sheet of population and area of years 2013 - 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census data filename follows this structure ACSDT5Y2013.B01003-Data\n",
    "# for all census data, columns: GEO_ID -> Census block id and B01003_001E -> Population\n",
    "# for census block group area, filename -> mecklenburg_county_census_block_group_area.xlsx\n",
    "# columns: GEOID, area_sq_km, area_sq_km, area_sq_m, area_statute_miles, area_us_survey_miles \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_list = [f\"Total_Population_Census_Block/ACSDT5Y{year}.B01003-Data.csv\" for year in range(2013, 2020)]\n",
    "area_file = \"mecklenburg_county_census_block_group_area.xlsx\"\n",
    "\n",
    "# Read census block area data\n",
    "area_df = pd.read_excel(area_file, dtype={'GEOID': str})\n",
    "\n",
    "# Initialize an empty DataFrame for population data\n",
    "population_df = pd.DataFrame()\n",
    "\n",
    "# Read and merge all population data files\n",
    "for year, file in zip(range(2013, 2020), file_list):\n",
    "    df = pd.read_csv(file, dtype={'GEO_ID': str})\n",
    "    df['GEO_ID'] = df['GEO_ID'].str.replace('1500000US', '', regex=False)\n",
    "    df = df[['GEO_ID', 'B01003_001E']].rename(columns={'GEO_ID': 'ID', 'B01003_001E': f'Pop_{year}'})\n",
    "    \n",
    "    if population_df.empty:\n",
    "        population_df = df\n",
    "    else:\n",
    "        population_df = population_df.merge(df, on='ID', how='outer')\n",
    "\n",
    "# Merge with area data\n",
    "merged_df = population_df.merge(area_df, left_on='ID', right_on='GEOID', how='left').drop(columns=['GEOID', 'NAMELSAD', 'INTPTLAT', 'INTPTLON', 'Shape_Length'])\n",
    "\n",
    "# Convert all columns to numeric\n",
    "merged_df = merged_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Save to an Excel file\n",
    "output_file = \"merged_census_data.xlsx\"\n",
    "merged_df.to_excel(output_file, index=False, float_format=\"%.10f\")\n",
    "\n",
    "print(f\"Merged data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make single sheet of population and area of years 2020 - 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_list = [f\"Total_Population_Census_Block/ACSDT5Y{year}.B01003-Data.csv\" for year in range(2020, 2024)]\n",
    "area_file = \"mecklenburg_county_census_block_group_area_2020.xlsx\"\n",
    "\n",
    "# Read census block area data\n",
    "area_df = pd.read_excel(area_file, dtype={'GEOID': str})\n",
    "\n",
    "# Initialize an empty DataFrame for population data\n",
    "population_df = pd.DataFrame()\n",
    "\n",
    "# Read and merge all population data files\n",
    "for year, file in zip(range(2020, 2024), file_list):\n",
    "    df = pd.read_csv(file, dtype={'GEO_ID': str})\n",
    "    df['GEO_ID'] = df['GEO_ID'].str.replace('1500000US', '', regex=False)\n",
    "    df = df[['GEO_ID', 'B01003_001E']].rename(columns={'GEO_ID': 'ID', 'B01003_001E': f'Pop_{year}'})\n",
    "    \n",
    "    if population_df.empty:\n",
    "        population_df = df\n",
    "    else:\n",
    "        population_df = population_df.merge(df, on='ID', how='outer')\n",
    "\n",
    "# Merge with area data\n",
    "merged_df = population_df.merge(area_df, left_on='ID', right_on='GEOID', how='left').drop(columns=['GEOID', 'NAMELSAD', 'INTPTLAT', 'INTPTLON', 'Shape_Length'])\n",
    "\n",
    "# Convert all columns to numeric\n",
    "merged_df = merged_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Save to an Excel file\n",
    "output_file = \"merged_census_data_2020.xlsx\"\n",
    "merged_df.to_excel(output_file, index=False, float_format=\"%.10f\")\n",
    "\n",
    "print(f\"Merged data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter revised image labels in between 2013 and 2019 including the edge years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file with correct data types\n",
    "file_path = \"revised_image_labels.xlsx\"\n",
    "df = pd.read_excel(file_path, dtype={\"Latitude\": str, \"Longitude\": str})\n",
    "\n",
    "# Convert 'Start' and 'End' columns to datetime format with day-first parsing\n",
    "df[\"Start\"] = pd.to_datetime(df[\"Start\"], dayfirst=True, errors=\"coerce\")\n",
    "df[\"End\"] = pd.to_datetime(df[\"End\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# Check the distribution of categories\n",
    "category_counts = df[\"Change\"].value_counts()\n",
    "print(\"Original Category Distribution:\\n\", category_counts)\n",
    "\n",
    "# Filter out rows where 'Start' or 'End' year is outside the range 2013-2019\n",
    "filtered_df = df[(df[\"Start\"].dt.year.between(2013, 2019)) & (df[\"End\"].dt.year.between(2013, 2019))]\n",
    "\n",
    "# Count occurrences of each 'Change' category\n",
    "change_counts = filtered_df[\"Change\"].value_counts()\n",
    "\n",
    "# Save the filtered data back to a new Excel file with proper date formatting\n",
    "output_path = \"image_labels_2013_to_2019.xlsx\"\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    filtered_df.to_excel(writer, index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Sheet1\"]\n",
    "\n",
    "    # Apply date format to ensure MM/DD/YYYY formatting in Excel\n",
    "    date_format = workbook.add_format({\"num_format\": \"mm/dd/yyyy\"})\n",
    "    start_col_idx = filtered_df.columns.get_loc(\"Start\")\n",
    "    end_col_idx = filtered_df.columns.get_loc(\"End\")\n",
    "\n",
    "    worksheet.set_column(start_col_idx, start_col_idx, 15, date_format)\n",
    "    worksheet.set_column(end_col_idx, end_col_idx, 15, date_format)\n",
    "\n",
    "# Print results\n",
    "print(f\"Filtered data saved to '{output_path}'\\n\")\n",
    "print(\"Change category counts after filtering:\")\n",
    "print(change_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link 2020 census block group to 2013 census block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "bg_2013 = gpd.read_file(\"mecklengburg_county_census_block_group_2013/mecklengburg_county_census_block_group_2013.shp\")\n",
    "bg_2020 = gpd.read_file(\"mecklengburg_county_census_block_group_2020/mecklengburg_county_census_block_group_2020.shp\")\n",
    "\n",
    "bg_2013 = bg_2013[['GEOID', 'geometry']].rename(columns={'GEOID': 'GEOID_2013'})\n",
    "bg_2020 = bg_2020[['GEOID', 'geometry']].rename(columns={'GEOID': 'GEOID_2020'})\n",
    "\n",
    "intersections = gpd.overlay(bg_2020, bg_2013, how='intersection')\n",
    "\n",
    "intersections['intersect_area'] = intersections.area\n",
    "\n",
    "max_overlap = (\n",
    "    intersections\n",
    "    .sort_values(by='intersect_area', ascending=False)\n",
    "    .drop_duplicates(subset='GEOID_2020')\n",
    "    .loc[:, ['GEOID_2020', 'GEOID_2013', 'intersect_area']]\n",
    ")\n",
    "# Convert all columns to numeric\n",
    "max_overlap = max_overlap.apply(pd.to_numeric, errors='coerce')\n",
    "max_overlap.to_excel(\"bg_2020_to_2013_mapping.xlsx\", index=False, float_format=\"%.14f\")\n",
    "\n",
    "# bg_2020_mapped = bg_2020.merge(max_overlap[['GEOID_2020', 'GEOID_2013']], on='GEOID_2020', how='left')\n",
    "# os.makedirs(\"mapped_shapefile\", exist_ok=True)\n",
    "# bg_2020_mapped.to_file(\"mapped_shapefile/bg_2020_with_2013_mapping.shp\", driver='ESRI Shapefile')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the blocks for prepared census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping between the census block groups was created above.\n",
    "# Now, let's actually change the blocks for prepared census data.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "prev_census_data_path = \"merged_census_data_2020.xlsx\"\n",
    "mapping_census_blocks_path = \"bg_2020_to_2013_mapping.xlsx\"\n",
    "\n",
    "census_df = pd.read_excel(prev_census_data_path)\n",
    "block_mapping_df = pd.read_excel(mapping_census_blocks_path)\n",
    "\n",
    "# Merge the census data with the mapping data\n",
    "merged_df = census_df.merge(block_mapping_df, left_on=\"ID\", right_on=\"GEOID_2020\", how=\"left\")\n",
    "\n",
    "# Locate the row with ID 371190001041\n",
    "target_row = merged_df[merged_df[\"ID\"] == 371190001041]\n",
    "\n",
    "# Create two new rows with split values\n",
    "row_1 = target_row.copy()\n",
    "row_2 = target_row.copy()\n",
    "\n",
    "# Update IDs\n",
    "row_1[\"GEOID_2013\"] = 371190001003\n",
    "row_2[\"GEOID_2013\"] = 371190001004\n",
    "\n",
    "# Define columns to split\n",
    "pop_columns = [f\"Pop_{year}\" for year in range(2020, 2024)]\n",
    "popden_columns = [f\"PopDen_{year}\" for year in range(2020, 2024)]\n",
    "\n",
    "# Apply the percentage splits\n",
    "row_1[pop_columns + popden_columns] *= 0.32\n",
    "row_2[pop_columns + popden_columns] *= 0.68\n",
    "\n",
    "# Remove the original row\n",
    "merged_df = merged_df[merged_df[\"ID\"] != 371190001041]\n",
    "\n",
    "# Append the two new rows\n",
    "final_df = pd.concat([merged_df, row_1, row_2], ignore_index=True)\n",
    "final_df = final_df.drop(columns=[\"ID\", \"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\", \"MTFCC\", \"FUNCSTAT\", \"ALAND\",\"AWATER\", \"GEOID_2020\", \"intersect_area\"]).rename(columns={'GEOID_2013': 'ID'})\n",
    "\n",
    "# Group by 'ID' and sum all other columns\n",
    "aggregated_df = final_df.groupby(\"ID\", as_index=False).sum()\n",
    "\n",
    "# # Save to an Excel file\n",
    "output_file = \"census_density_calculate_2020_with_2013_blocks.xlsx\"\n",
    "aggregated_df.to_excel(output_file, index=False, float_format=\"%.10f\")\n",
    "print(f\"Merged data saved to {aggregated_df}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge these census density calculated 2020 with census density calculated to make data from 2013 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "census_density_cal_2013 = \"census_density_calculated.xlsx\"\n",
    "census_density_cal_2020 = \"census_density_calculate_2020_with_2013_blocks.xlsx\"\n",
    "\n",
    "density_cal_2013_df = pd.read_excel(census_density_cal_2013)\n",
    "density_cal_2020_df = pd.read_excel(census_density_cal_2020)\n",
    "\n",
    "# Merge the census data with the mapping data\n",
    "merged_df = density_cal_2013_df.merge(density_cal_2020_df, left_on=\"ID\", right_on=\"ID\", how=\"left\")\n",
    "\n",
    "merged_df = merged_df.drop(columns=['area_sq_m_x', 'area_statute_miles', 'area_us_survey_miles', 'Shape_Area_y', 'area_sq_m_y'])\n",
    "\n",
    "new_order = ['ID', 'Pop_2013', 'Pop_2014', 'Pop_2015', 'Pop_2016', 'Pop_2017',\n",
    "       'Pop_2018', 'Pop_2019', 'Pop_2020', 'Pop_2021', 'Pop_2022', 'Shape_Area_x', 'area_sq_km', 'PopDen_2013',\n",
    "       'PopDen_2014', 'PopDen_2015', 'PopDen_2016', 'PopDen_2017',\n",
    "       'PopDen_2018', 'PopDen_2019','PopDen_2020', 'PopDen_2021',\n",
    "       'PopDen_2022', 'PopDen_2023']\n",
    "merged_df = merged_df.reindex(columns=new_order)\n",
    "# Save to an Excel file\n",
    "output_file = \"census_density_calculated_2013_2023.xlsx\"\n",
    "merged_df.to_excel(output_file, index=False, float_format=\"%.10f\")\n",
    "print(f\"Merged data saved to {merged_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter revised image labels in between 2013 and 2023 including the edge years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file with correct data types\n",
    "file_path = \"revised_image_labels.xlsx\"\n",
    "df = pd.read_excel(file_path, dtype={\"Latitude\": str, \"Longitude\": str})\n",
    "\n",
    "# Convert 'Start' and 'End' columns to datetime format with day-first parsing\n",
    "df[\"Start\"] = pd.to_datetime(df[\"Start\"], dayfirst=True, errors=\"coerce\")\n",
    "df[\"End\"] = pd.to_datetime(df[\"End\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# Check the distribution of categories\n",
    "category_counts = df[\"Change\"].value_counts()\n",
    "print(\"Original Category Distribution:\\n\", category_counts)\n",
    "\n",
    "# Filter out rows where 'Start' or 'End' year is outside the range 2013-2023\n",
    "filtered_df = df[(df[\"Start\"].dt.year.between(2013, 2023)) & (df[\"End\"].dt.year.between(2013, 2023))]\n",
    "\n",
    "# Count occurrences of each 'Change' category\n",
    "change_counts = filtered_df[\"Change\"].value_counts()\n",
    "\n",
    "# Save the filtered data back to a new Excel file with proper date formatting\n",
    "output_path = \"image_labels_2013_to_2023.xlsx\"\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    filtered_df.to_excel(writer, index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Sheet1\"]\n",
    "\n",
    "    # Apply date format to ensure MM/DD/YYYY formatting in Excel\n",
    "    date_format = workbook.add_format({\"num_format\": \"mm/dd/yyyy\"})\n",
    "    start_col_idx = filtered_df.columns.get_loc(\"Start\")\n",
    "    end_col_idx = filtered_df.columns.get_loc(\"End\")\n",
    "\n",
    "    worksheet.set_column(start_col_idx, start_col_idx, 15, date_format)\n",
    "    worksheet.set_column(end_col_idx, end_col_idx, 15, date_format)\n",
    "\n",
    "# Print results\n",
    "print(f\"Filtered data saved to '{output_path}'\\n\")\n",
    "print(\"Change category counts after filtering:\")\n",
    "print(change_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate image labels with population density data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "census_file = \"image_labels_2013_to_2023.xlsx\"\n",
    "pop_density_file = \"census_density_calculated_2013_2023.xlsx\"\n",
    "\n",
    "# Load the census data\n",
    "df_census = pd.read_excel(census_file, dtype={\"Latitude\": str, \"Longitude\": str})\n",
    "\n",
    "# Load the population density data\n",
    "df_pop = pd.read_excel(pop_density_file, dtype={\"ID\": str})  # Ensure ID is string to match\n",
    "\n",
    "# Convert 'Start' and 'End' columns to datetime format\n",
    "df_census[\"Start\"] = pd.to_datetime(df_census[\"Start\"], dayfirst=True, errors=\"coerce\")\n",
    "df_census[\"End\"] = pd.to_datetime(df_census[\"End\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# Extract years from 'Start' and 'End' columns\n",
    "df_census[\"Start_Year\"] = df_census[\"Start\"].dt.year\n",
    "df_census[\"End_Year\"] = df_census[\"End\"].dt.year\n",
    "\n",
    "# Function to get population density difference\n",
    "def get_pop_density_diff(row):\n",
    "    block_id = str(row[\"ID\"])  # Ensure ID is string to match both dataframes\n",
    "    start_year = row[\"Start_Year\"]\n",
    "    end_year = row[\"End_Year\"]\n",
    "\n",
    "    # Fetch the population density values for the start and end years\n",
    "    pop_start = df_pop.loc[df_pop[\"ID\"] == block_id, f\"PopDen_{start_year}\"]\n",
    "    pop_end = df_pop.loc[df_pop[\"ID\"] == block_id, f\"PopDen_{end_year}\"]\n",
    "\n",
    "    # Check if values exist before computing the difference\n",
    "    if not pop_start.empty and not pop_end.empty:\n",
    "        popden_diff = pop_end.values[0] - pop_start.values[0]\n",
    "        if pop_end.values[0] == 0 and pop_start.values[0] == 0:\n",
    "            popden_diff_percentage = 0\n",
    "        elif pop_start.values[0] == 0:\n",
    "            popden_diff_percentage = 100\n",
    "        else:\n",
    "            popden_diff_percentage = (popden_diff * 100) / pop_start.values[0]\n",
    "        return popden_diff, popden_diff_percentage\n",
    "    return None, None\n",
    "\n",
    "# Apply the function to compute population density difference\n",
    "# df_census[\"PopDen_Diff\"] = df_census.apply(get_pop_density_diff, axis=1)\n",
    "df_census[[\"PopDen_Diff\", \"PopDen_Diff_Percentage\"]] = df_census.apply(\n",
    "    lambda row: pd.Series(get_pop_density_diff(row)), axis=1\n",
    ")\n",
    "df_census.drop(columns=['Start_Year', 'End_Year'], inplace=True)\n",
    "# Save the updated census data with the new column\n",
    "output_file = \"image_labels_2013_to_2023_with_pop_den.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "    df_census.to_excel(writer, index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Sheet1\"]\n",
    "\n",
    "    # Apply date format to ensure MM/DD/YYYY formatting in Excel\n",
    "    date_format = workbook.add_format({\"num_format\": \"mm/dd/yyyy\"})\n",
    "    start_col_idx = df_census.columns.get_loc(\"Start\")\n",
    "    end_col_idx = df_census.columns.get_loc(\"End\")\n",
    "\n",
    "    worksheet.set_column(start_col_idx, start_col_idx, 15, date_format)\n",
    "    worksheet.set_column(end_col_idx, end_col_idx, 15, date_format)\n",
    "\n",
    "# Print message\n",
    "print(f\"Updated census data saved to '{output_file}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub sample the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "excel_file = \"image_labels_2013_to_2023_with_pop_den.xlsx\"\n",
    "df = pd.read_excel(excel_file, dtype={\"Latitude\": str, \"Longitude\": str})\n",
    "\n",
    "# Convert Start and End to datetime\n",
    "df['Start'] = pd.to_datetime(df['Start'], dayfirst=True, errors='coerce').dt.date\n",
    "df['End'] = pd.to_datetime(df['End'], dayfirst=True, errors='coerce').dt.date\n",
    "\n",
    "# Exclude re-greening\n",
    "df = df[df[\"Change\"] != \"re-greening\"]\n",
    "\n",
    "category_counts = df[\"Change\"].value_counts()\n",
    "print(\"Original Category Distribution excluding re-greening:\\n\", category_counts)\n",
    "\n",
    "# Separate \"No Change\"\n",
    "df_no_change = df[df[\"Change\"] == \"No Change\"].reset_index(drop=True)\n",
    "\n",
    "# Sample every 10th row from \"No Change\"\n",
    "df_no_change_sampled = df_no_change.iloc[::10].head(5580)\n",
    "\n",
    "# Keep other categories as-is\n",
    "df_others = df[df[\"Change\"] != \"No Change\"]\n",
    "\n",
    "# Combine datasets\n",
    "df_final_sampled = pd.concat([df_no_change_sampled, df_others], ignore_index=True)\n",
    "\n",
    "# Duplicate \"re-capital\" rows two times\n",
    "df_re_capital = df_final_sampled[df_final_sampled[\"Change\"] == \"re-capital\"]\n",
    "df_re_capital_duplicated = pd.concat([df_re_capital] * 2, ignore_index=True)\n",
    "\n",
    "# Remove original \"re-capital\" rows and add duplicated rows\n",
    "df_final_sampled = df_final_sampled[df_final_sampled[\"Change\"] != \"re-capital\"]\n",
    "df_final_sampled = pd.concat([df_final_sampled, df_re_capital_duplicated], ignore_index=True)\n",
    "\n",
    "# Sort the dataframe\n",
    "df_final_sampled = df_final_sampled.sort_values(\n",
    "    by=[\"ID\", \"Latitude\", \"Longitude\", \"Heading\", \"Start\"],\n",
    "    ascending=[True, True, True, True, True]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Save to Excel with formatting\n",
    "output_excel = \"every_10_nochange_recapital_doubled_2013_2023_pop_den_labels.xlsx\"\n",
    "with pd.ExcelWriter(output_excel, engine=\"xlsxwriter\") as writer:\n",
    "    df_final_sampled.to_excel(writer, index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Sheet1\"]\n",
    "\n",
    "    # Date formatting\n",
    "    date_format = workbook.add_format({\"num_format\": \"mm/dd/yyyy\"})\n",
    "    worksheet.set_column(df_final_sampled.columns.get_loc(\"Start\"), df_final_sampled.columns.get_loc(\"Start\"), 15, date_format)\n",
    "    worksheet.set_column(df_final_sampled.columns.get_loc(\"End\"), df_final_sampled.columns.get_loc(\"End\"), 15, date_format)\n",
    "\n",
    "# Final distribution check\n",
    "print(df_final_sampled[\"Change\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
